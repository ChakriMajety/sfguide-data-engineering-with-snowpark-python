{"cells":[{"cell_type":"markdown","metadata":{},"source":["The purpose of this Demo is to provide a simple framework for Snowpark Hands on Labs that does not rely on a local environment (which can require python installation and complex steps to accomodate many environment types). Hex is used because it offers a 14 day free trial which in conjunction with a Snowflake account (trial) can be used for scaling hands on labs to many people. \n","\n","In this Lab we will Take the Thoughtspot Fantasy football dataset and train a machine learning model that will predict the point spread at the end of regulation for football games. \n","\n","In this lab we will be following the standard machine learning workflow. As football games are very difficult to predict this lab is not designed to produce a model that will accurately predict NFL games. \n","\n","Framework:\n","\n","1. Exploratory Data Analysis\n","2. Data Cleansing and Data Engineering\n","3. Machine Learning Model Training\n","4. Model Deployment"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Instructions\n","\n","1. Head over to the ,[Snowflake](https://signup.snowflake.com/), sign-up page and register for a free account in AWS-West (Oregon) Enterprise Edition. Once you've registered, you'll get an email that will bring you to Snowflake so that you can sign in. Make sure to Activate your account and pick a username and password that you will remember. This will be important for logging in later on. \n","2. Once you've logged into your Snowflake account, you'll land on the ,`Learn`, page. Simply navigate to the ,`Admin`, tab on the left and click ,`Partner connect`,. In the search bar at the top, type in ,`Hex`,, and you should see the Hex partner connect tile appear. Clicking on the tile will bring up a new screen, and all you have to do is to press the connect button in the lower right corner. After this, you'll see a new screen confirming that your account has been created and from here you can click ,`Activate`,. In the Hex Workspace page choose a workspace that. you will remember. You may use your company name however it may be best to create a separate workspace for the purposes of this lab.\n","\n","## Workflow roadblocks\n","\n","The following issues may occur if you have an existing Hex account and you're not an Admin in that org.\n","\n","**Unauthorized error: **If you have an existing Hex account that was created with a password and username, you may run into an \"Unauthorized\" error when activating your workspace in Partner Connect. If this is your experience, head over to [hex.tech](https://hex.tech/) and login with your password and username.\n","\n","**Plan upgrade: **If you are an existing Hex user currently on a Community plan, you may encounter an issue that will prevent you from using Partner Connect. If you're unclear on what Hex plan you are on, feel free to reach out to [support@hex.tech](mailto:support@hex.tech). If you are the Admin of your organization, you can see your plan under Manage plan on the Settings page. To extend a trial, email [support@hex.tech](mailto:support@hex.tech) with the subject \"VHOL trial extension.\"\n","\n","**Role privileges: **If you do not have an Editor role or higher, you won't be able to create data connections in your workspace. To upgrade your role, contact your workspace Admin. You can find out who this is by navigating to Settings -> Users & groups within your Hex workspace._If you're still encountering issues, or encounter any issues other than the ones listed above, please contact our support team _[support@hex.tech](mailto:support@hex.tech)_ with the subject \"VHOL\" for priority support._\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"/api/v1/file/ce983376-9e89-4b04-97b5-66e6db63d2b7\" width=\"0\" />\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Creating a workspace\n","\n","Once activated, you'll be brought over to Hex and will be prompted to create/name your new workspace. After you've named your workspace, you'll be brought to the [projects](https://learn.hex.tech/docs/getting-started/intro-to-projects#projects-home) page where you can create new projects, import existing projects (Hex or Jupyter) as well as navigate to other sections of your workspace.\n","\n","## Enabling ORGADMIN\n","\n","We'll revisit your newly created workspace in a bit, but for now, head back over to Snowflake. Let's navigate to the `Admin` tab again but this time select `Users & roles`. From here, you should see 3 users with one of them being named `PC_HEX_USER`. This is the user that was created when you activated Hex with partner connect. We'll need to activate the `ORGADMIN` role for this user. Select `PC_HEX_USER`, and at the bottom of the page you'll see a section to grant new roles.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"/api/v1/file/b97fc1cc-701c-4644-83ae-6814740ca793\" width=\"600\" />\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Click on grant role, which will open a window to grant roles to the `PC_HEX_USER` account. In the `Role to grant` dropdown, you'll see the role `ORGADMIN`. Select this role and then click `Grant`. We will revisit this step in a later section.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"/api/v1/file/65a5c835-7284-453b-9f66-c98c63e51812\" width=\"600\" />\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Import Hex Notebook\n","\n","You have been emailed a file named Fantasy_Football_Demo.yaml\n","\n","We will want to import this into our hex notebook environment.\n","\n","In order to Import this please navigate to your Hex workspace and click \"Import\"\n","\n","Select the YAML file and import. \n","\n","The Gifs included in the workspace may not import correctly, feel free to delete these cells. I will share my notebook with attendees directly to preserve these gifs. \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Before we dive into the code, we'll need to:\n","\n","1. Change our compute profile to run Python 3.8\n","2. Import our Snowflake data connection\n","\n","Which we can do all from the left control panel. To change the compute profile, click on the Environments tab represented by a cube. At the top of this section you'll see the compute profile portion at the top. Click on the `Image` dropdown and select Python 3.8.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"/api/v1/file/a9b20a01-12d1-42e5-835a-76110eb4175a\" width=\"600\" />\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Next we can import our Snowflake data connection by heading over to the `Data sources` tab represented by a database icon with a lightning bolt. At the bottom of this section, you'll see a portion that says available workspace connections and you should see one that says Snowflake. \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"/api/v1/file/54e60add-0c9b-42b5-9ab7-efd3477b58d7\" width=\"600\" />\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Accept Conda Terms and Conditions\n","\n","At this point, you are going to run into an error when running the cell that defines the UDF. This is because we haven't yet accepted the Anaconda terms and conditions. In this step, we'll go over how to accept the [Anaconda terms and conditions enabled by the ORGADMIN](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-packages.html#using-third-party-packages-from-anaconda) role we granted ourselves access to earlier. To do this, navigate back to Snowflake and click on your username in the top left corner. You'll see a section that will allow you to switch to the `ORGADMIN` role. Once switched over, navigate to the `Admin` tab and select `Billing & Terms`. From here, you will see a section that will allow you to accept the anaconda terms and conditions. Once this is done, you can head back over to Hex and run the cell that defines our UDTF.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"/api/v1/file/cb98851c-1deb-467f-8bb2-826b9404e39d\" width=\"600\" />\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Getting Marketplace Data\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["In the Upper Left click your user name, navigate to Switch ROle and select account Admin.\n","\n","Navigate to the Snowflake Marketplace tab. \n","\n","Search for Fantasy Football Data.\n","\n","Navigate to the listing shown in the image below:\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"/api/v1/file/b256bb72-784d-4937-9b85-ed27f68637f4\" width=\"600\" />\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Click Get Data.\n","\n","Expand the options tab and ensure that you select ACCOUNTADMIN and PC_HEX_ROLE from the roles dropdown list.\n","\n","Name the database FANTASY_FOOTBALL\n","\n","Click Get\n","\n","Click DONE\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import sklearn\n","from snowflake.snowpark import Session\n","from snowflake.snowpark.types import Variant\n","from snowflake.snowpark import Window\n","import matplotlib.pyplot as plt\n","from snowflake.snowpark import functions as F\n","import hextoolkit\n","from snowflake.snowpark import Window\n","from copy import copy\n","from PIL import Image\n","import PIL\n","import requests\n","from io import BytesIO"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"/api/v1/file/e79956b0-6778-42e8-a31b-fde164d5e8dd\" width=\"1261\" />\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Exploratory Data Analysis and Cleanup"]},{"cell_type":"markdown","metadata":{},"source":["### Get Data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["print(\"EXample\")\n","#Get our Session Object \n","hex_snowflake_conn = hextoolkit.get_data_connection(\"Snowflake\")\n","hex_snowpark_session = hex_snowflake_conn.get_snowpark_session()\n","\n","hex_snowpark_session.sql_simplifier_enabled=True\n","\n","#Assign Tables in our Datashare to Dataframe Objects\n","pbp = hex_snowpark_session.table(\"FANTASY_FOOTBALL.NFL2022.PBP\")\n","pbp_pass = hex_snowpark_session.table(\"FANTASY_FOOTBALL.NFL2022.PBP_PASSING\")\n","pbp_rush = hex_snowpark_session.table(\"FANTASY_FOOTBALL.NFL2022.PBP_RUSHING\")\n","pbp_receiving = hex_snowpark_session.table(\"FANTASY_FOOTBALL.NFL2022.PBP_RECEIVING\")\n","roster = hex_snowpark_session.table(\"FANTASY_FOOTBALL.NFL2022.ROSTER\")\n","schedule = hex_snowpark_session.table(\"FANTASY_FOOTBALL.NFL2022.SCHEDULE\")\n","teams = hex_snowpark_session.table(\"FANTASY_FOOTBALL.NFL2022.TEAMS\")"]},{"cell_type":"markdown","metadata":{},"source":["## Explore Tables\n","\n","Our First code block will show us the basics of Snowpark. \n","1. Query the Entire Play-by-Play Table\n","\n","2. Show only Plays where the Seattle Seahawks were on offense during week 13.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Show Our Full Table\n","pbp.show(3)\n","\n","\n","# Show only Games Played by Seattle, during Week 13 where Seattle was on Offense\n","pbp.filter((F.col(\"HOME_TEAM\") == \"SEA\") | (F.col(\"AWAY_TEAM\") == \"SEA\"))\\\n","   .filter(F.col(\"WEEK\") == 13)\\\n","   .filter(F.col(\"POSTEAM\") == \"SEA\")\\\n","   .select([\"GAME_DATE\", \"QTR\", \"DOWN\", \"DESC\", \"SCORE_DIFFERENTIAL\"])\\\n",".show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["pbp_pass.show(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["pbp_rush.show(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["pbp_receiving.show(3)"]},{"cell_type":"markdown","metadata":{},"source":["# Write a function that Gets the schedule for a given team\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Lets Get a list of teams to populate a team select b_hex_os\n","distinct_teams = teams.select(\"TEAM_ABBR\").distinct().to_pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import json as _hex_json\n","team_select = _hex_pks.kernel_execution.input_cell.run_dropdown_dynamic(args=_hex_types.DropdownDynamicArgs.from_dict({**_hex_json.loads(\"{\\\"dataframe_column\\\":\\\"TEAM_ABBR\\\",\\\"ui_selected_value\\\":\\\"SEA\\\"}\"), **{_hex_json.loads(\"\\\"options_variable\\\"\"):_hex_kernel.variable_or_none(\"distinct_teams\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()))\n","\n","import json as _hex_json\n","_hex_pks.kernel_execution.input_cell.filled_dynamic_value(args=_hex_types.FilledDynamicValueArgs.from_dict({**_hex_json.loads(\"{\\\"variable_name\\\":\\\"distinct_teams\\\",\\\"dataframe_column\\\":\\\"TEAM_ABBR\\\",\\\"max_size\\\":10000,\\\"max_size_in_bytes\\\":5242880}\"), **{_hex_json.loads(\"\\\"variable\\\"\"):_hex_kernel.variable_or_none(\"distinct_teams\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()))"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["teams.select(\"TEAM_ABBR\", \"TEAM_NAME\", \"TEAM_DIVISION\", \"TEAM_LOGO_WIKIPEDIA\").show(3)\n","\n","\n","def get_team_logo(abbr: str) -> PIL.PngImagePlugin.PngImageFile:\n","\n","    team_image_url = (\n","        teams.filter(F.col(\"TEAM_ABBR\") == abbr)\n","        .select(\"TEAM_LOGO_WIKIPEDIA\")\n","        .collect()[0][0]\n","    )\n","    response = requests.get(team_image_url)\n","    img = Image.open(BytesIO(response.content))\n","    return img\n","\n","\n","get_team_logo(team_select)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["schedule.select(\"HOME_TEAM\", \"AWAY_TEAM\", \"WEEK\", \"HOME_SCORE\", \"AWAY_SCORE\").show(5)\n","\n","\n","def get_schedule(abbr):\n","    home_col = F.col(\"HOME_TEAM\")\n","    away_col = F.col(\"AWAY_TEAM\")\n","    df = schedule.filter((home_col == abbr) | (away_col == abbr)).order_by(\n","        F.col(\"WEEK\").asc()\n","    )\n","    return df\n","\n","\n","get_schedule(team_select).select(\"HOME_TEAM\", \"AWAY_TEAM\", \"WEEK\").show(18)\n","\n","\n","schedule_cols_to_drop = [\n","    \"SEASON\",\n","    \"GAME_TYPE\",\n","    \"WEEKDAY\",\n","    \"GAMETIME\",\n","    \"AWAY_SCORE\",\n","    \"HOME_SCORE\",\n","    \"LOCATION\",\n","    \"RESULT\",\n","    \"TOTAL\",\n","    \"OVERTIME\",\n","    \"OLD_GAME_ID\",\n","    \"GSIS\",\n","    \"NFL_DETAIL_ID\",\n","    \"PFR\",\n","    \"PFF\",\n","    \"ESPN\",\n","    \"AWAY_REST\",\n","    \"HOME_REST\",\n","    \"TOTAL_LINE\",\n","    \"UNDER_ODDS\",\n","    \"OVER_ODDS\",\n","    \"TEMP\",\n","    \"WIND\",\n","    \"AWAY_QB_ID\",\n","    \"HOME_QB_ID\",\n","    \"AWAY_QB_NAME\",\n","    \"HOME_QB_NAME\",\n","    \"REFEREE\",\n","    \"STADIUM_ID\",\n","    \"STADIUM\",\n","    \"ROOF\",\n","    \"SURFACE\",\n","    \"AWAY_COACH\",\n","    \"HOME_COACH\",\n","    \"DIV_GAME\",\n","]\n","\n","\n","schedule = schedule.drop(schedule_cols_to_drop)\n","\n","schedule.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#Our Schedule Is missing scores we want to predict the winner of any given game and the spread. To do this we will need to compute a new table\n","#That will aggregate scoring plays from our play by play table.\n"]},{"cell_type":"markdown","metadata":{},"source":["### How many Games does our Database Contain?\n","\n","1. Use a dataframe method\n","\n","2. Use a Snowpark function\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["num_games_dataframe_methods = schedule.select(\"GAME_ID\").distinct().count()\n","print(num_games_dataframe_methods)\n","\n","schedule.select(F.count_distinct(\"GAME_ID\")).show()"]},{"cell_type":"markdown","metadata":{},"source":["### Compute Final Score Differential\n","\n","Our Data is incomplete we are missing the final score for any game. Our play by play data includes a \"Score_DIFFERENTIAL\" column. This column is the difference in points between two teams. Lets compute the point differential on the final play of a game.\n","\n","****\n","\n","**Note:**Our data does not include field goals as plays. This means that field goals that end a game will not be included in our final point total. Team wins may vary.We also are missing overtime data which means our data will include many ties that are games that ended regulation tied.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Lets Get the Final Score Differential for each game, this will give us a target variable\n","\n","# We will use a window function to compute the final play of a game.\n","# This will be the Max PLAY_ID for a given Game.\n","# Partitioning by game and finding the MAX Play id will allow us to find the final score at the end of regulation.\n","\n","w = Window.partitionBy([\"GAME_ID\"]).order_by(F.col(\"PLAY_ID\").desc())\n","\n","\n","# Calculate the final play of a game\n","final_play = F.max(F.col(\"PLAY_ID\")).over(w)\n","\n","\n","winning_team = (\n","    F.when(F.col(\"SCORE_DIFFERENTIAL\") > 0, F.col(\"POSTEAM\"))\n","    .when(F.col(\"SCORE_DIFFERENTIAL\") < 0, F.col(\"DEFTEAM\"))\n","    .otherwise(F.lit(\"TIE\"))\n",")\n","\n","losing_team = (\n","    F.when(F.col(\"SCORE_DIFFERENTIAL\") > 0, F.col(\"DEFTEAM\"))\n","    .when(F.col(\"SCORE_DIFFERENTIAL\") < 0, F.col(\"POSTEAM\"))\n","    .otherwise(F.lit(\"TIE\"))\n",")\n","\n","final_point_differential = (\n","    pbp.withColumn(\"FINAL_PLAY\", final_play)\n","    .with_column(\"WINNER\", winning_team)\n","    .with_column(\"LOSER\", losing_team)\n","    .filter(F.col(\"PLAY_ID\") == F.col(\"final_play\"))\n","    .drop_duplicates(\"PBP_ID\")\n","    .drop_duplicates(\"GAME_ID\")\n",")\n","\n","\n","final_point_differential.drop_duplicates(\"PBP_ID\").filter(F.col(\"WEEK\") == 1).select(\n","    \"GAME_ID\",\"POSTEAM\", \"DEFTEAM\", \"SCORE_DIFFERENTIAL\", \"WINNER\", \"LOSER\"\n",").order_by(\"GAME_ID\").show(5)"]},{"cell_type":"markdown","metadata":{},"source":["### Update Schedule Table\n","\n","We have the Final Play of Each Game, Now we want to update our Schedule Table with Scores for each game playedLets Join the new Play by Play Dataframe to Our Schedule Table.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#Now Lets Join our play by play calculated point differential with our schedule column to add winners and the spread \n","\n","\n","# Take all schedule Table columns add the winner, loser and differential columns from our pbp table\n","\n","schedule_cols = [schedule.col(\"*\")] + [\n","    F.col(\"WINNER\"),\n","    F.col(\"LOSER\"),\n","    F.abs(F.col(\"SCORE_DIFFERENTIAL\")).alias(\"SCORE_DIFFERENTIAL\"),\n","]\n","\n","# Home differential is the point differential relative to the home team, positive if home team won, negative if home team lost \n","home_differential = F.when(\n","    F.col(\"WINNER\") != F.col(\"HOME_TEAM\"), F.lit(-1) * F.col(\"SCORE_DIFFERENTIAL\")\n",").otherwise(F.col(\"SCORE_DIFFERENTIAL\"))\n","\n","\n","# Schedule Updated Is Now Our Schedule table including differential and winner/loser information\n","schedule_updated = (\n","    schedule.join(\n","        final_point_differential,\n","        schedule.GAME_ID == final_point_differential.GAME_ID,\n","        rsuffix=\"rhs\",\n","    )\n","    .select(schedule_cols)\n","    .with_column(\"home_differential\", home_differential)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Results by Team By Week\n","\n","Now that we have our schedule updated with Winners and losers. We want a table that shows an individual teams results.\n","This will be used to compute winning percentage.\n","\n","| Result      | Value |\n","|------------|-------|\n","| Win        | 1     |\n","| Loss       | 0     |\n","| Tie        | 0.5   |\n","\n","Here we are getting into more complex tranformations. We will use compute two dataframes that assign values for the Home and Away team separately. Then we will Union these two tables to get a final result. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["results_by_team_week = (\n","    schedule_updated.withColumn(\n","        \"results\",\n","        F.when(schedule_updated.WINNER == schedule_updated.HOME_TEAM, 1)\n","        .when(schedule_updated.WINNER == schedule_updated.AWAY_TEAM, 0)\n","        .otherwise(0.5),\n","    )\n","    .groupBy(\"HOME_TEAM\", \"WEEK\")\n","    .agg(F.sum(\"results\").alias(\"results\"))\n","    .union(\n","        schedule_updated.withColumn(\n","            \"results\",\n","            F.when(schedule_updated.WINNER == schedule_updated.AWAY_TEAM, 1)\n","            .when(schedule_updated.WINNER == schedule_updated.HOME_TEAM, 0)\n","            .otherwise(0.5),\n","        )\n","        .groupBy(\"AWAY_TEAM\", \"WEEK\")\n","        .agg(F.sum(\"results\").alias(\"results\"))\n","    )\n","    .groupBy(\"HOME_TEAM\", \"WEEK\")\n","    .agg(F.sum(\"results\").alias(\"results\"))\n",")\n","\n","results_by_team_week = results_by_team_week.with_column_renamed(F.col(\"HOME_TEAM\"),\"TEAM\")\n","\n","results_by_team_week.filter(F.col(\"WEEK\") == 1).order_by(\"TEAM\").show()"]},{"cell_type":"markdown","metadata":{},"source":["# Compute Winning Percentage\n","\n","We Now want to compute Winning Percentage for each team for each week in our dataset.\n","\n","We can do this by using window functions over a window partitioned by team and ordered by week.\n","<br>\n","<br/>\n","\n","Win Percentage is computed as follows:\n","<br>\n","<br/>\n","\n","$$Winning Percentage = \\frac{(Wins \\times 1) + (Losses \\times 0) + (Ties \\times 0.5)}{GamesPlayed}$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Create a new column called \"WINS\" that is 1 if the result is a win and 0 otherwise\n","results_by_team_week = results_by_team_week.withColumn(\n","    \"WINS\", F.when(results_by_team_week[\"RESULTS\"] == 1, 1).otherwise(0)\n",").withColumn(\n","    \"TIES\", F.when(results_by_team_week[\"RESULTS\"] == 0.5, 1).otherwise(0) \n",").withColumn(\n","    \"LOSSES\", F.when(results_by_team_week[\"RESULTS\"] == 0, 1).otherwise(0)\n",")\n","\n","\n","\n","# Define a window function that sorts the data by TEAM and WEEK\n","window = Window.partitionBy(\"TEAM\").orderBy(\"WEEK\")\n","\n","# Calculate the running total of wins, ties, and losses for each team using the window function\n","results_by_team_week = results_by_team_week.withColumn(\n","    \"RUNNING_WINS\", F.sum(\"WINS\").over(window)\n",")\n","results_by_team_week = results_by_team_week.withColumn(\n","    \"RUNNING_TIES\", F.sum(\"TIES\").over(window)\n",")\n","results_by_team_week = results_by_team_week.withColumn(\n","    \"RUNNING_LOSSES\", F.sum(\"LOSSES\").over(window)\n",")\n","\n","wins = F.col(\"RUNNING_WINS\")\n","ties = F.col(\"RUNNING_TIES\")\n","losses = F.col(\"RUNNING_LOSSES\")\n","\n","win_percent = (wins * 1 + ties * 0.5) / (losses + wins + ties)\n","\n","results_by_team_week = results_by_team_week.withColumn(\"Win_percentage\", win_percent)\n","\n","\n","#Add another column with lagged win percentage, this will be a feature in our ML pipeline and represents the \n","#Win percent of a team going in to any given game.\n","results_by_team_week = results_by_team_week.withColumn(\n","    \"LAGGED_WIN_PERCENTAGE\",\n","    F.when(\n","        F.lag(\"WIN_PERCENTAGE\").over(Window.partitionBy(\"TEAM\").orderBy(\"WEEK\")).isNull(),\n","        0,\n","    ).otherwise(F.lag(\"WIN_PERCENTAGE\").over(Window.partitionBy(\"TEAM\").orderBy(\"WEEK\"))),\n",")\n","\n","\n","# Display the results_by_team_week dataframe, sorted by \"TEAM\"\n","results_by_team_week.filter(F.col(\"TEAM\") == team_select).select(\n","    \"TEAM\", \"WEEK\", \"WIN_PERCENTAGE\",\"LAGGED_WIN_PERCENTAGE\"\n",").order_by(\"WEEK\").show(18)"]},{"cell_type":"markdown","metadata":{},"source":["## Join to Schedule Table\n","\n","Now we must join our winning percentage back to our schedule table. We must individually do the home and away teams.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Remove unnecessary columns from results_by_team_week\n","results_by_team_week = results_by_team_week.drop(\n","    [\n","        \"RESULTS\",\n","        \"WINS\",\n","        \"TIES\",\n","        \"LOSSES\",\n","        \"RUNNING_WINS\",\n","        \"RUNNING_TIES\",\n","        \"RUNNING_LOSSES\",\n","    ]\n",")\n","\n","# Define the join condition for the two dataframes\n","home_join_condition = (schedule_updated.WEEK == results_by_team_week.WEEK) & (\n","    schedule_updated.HOME_TEAM == results_by_team_week.TEAM\n",")\n","\n","# Join the two dataframes using the \"TEAM\" column and give the joined dataframe a new name\n","schedule_updated = schedule_updated.join(\n","    results_by_team_week, home_join_condition, rsuffix=\"_THROWAWAY\"\n",")\n","\n","# Remove the unnecessary column created by the join\n","schedule_updated = schedule_updated.drop(\"WEEK_THROWAWAY\")\n","\n","# Rename the \"WIN_PERCENTAGE\" column to \"HOME_TEAM_WIN_PERCENTAGE\"\n","schedule_updated = schedule_updated.withColumnRenamed(\n","    \"WIN_PERCENTAGE\", \"HOME_TEAM_WIN_PERCENTAGE\"\n",")\n","\n","schedule_updated = schedule_updated.withColumnRenamed(\"LAGGED_WIN_PERCENTAGE\",\"HOME_LAGGED_WIN_PERCENTAGE\")\n","\n","# Remove the \"TEAM\" column from the dataframe\n","schedule_updated = schedule_updated.drop(\"TEAM\")\n","\n","# Display the resulting dataframe, sorted by \"GAME_ID\"\n","schedule_updated.filter(F.col(\"HOME_TEAM\") == team_select).order_by(\"GAME_ID\").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Make a copy of the results dataframe\n","results_by_team_week_2 = copy(results_by_team_week)\n","\n","# Join the schedule_updated dataframe with the results dataframe using the away team and week\n","away_join_condition = (schedule_updated.WEEK == results_by_team_week_2.WEEK) & (\n","    schedule_updated.AWAY_TEAM == results_by_team_week_2.TEAM\n",")\n","schedule_updated = schedule_updated.join(\n","    results_by_team_week_2, away_join_condition, rsuffix=\"_THROWAWAY\"\n",")\n","\n","# Rename the \"WIN_PERCENTAGE\" column and drop the \"WEEK_THROWAWAY\" and \"TEAM\" columns\n","schedule_updated = schedule_updated.withColumnRenamed(\n","    \"WIN_PERCENTAGE\", \"AWAY_TEAM_WIN_PERCENTAGE\"\n",")\n","schedule_updated = schedule_updated.withColumnRenamed(\n","    \"LAGGED_WIN_PERCENTAGE\", \"AWAY_LAGGED_WIN_PERCENTAGE\"\n",")\n","schedule_updated = schedule_updated.drop(\"WEEK_THROWAWAY\", \"TEAM\")\n","\n","# Show the data for week 15\n","schedule_updated.filter(\n","    (F.col(\"HOME_TEAM\") == team_select) | (F.col(\"AWAY_TEAM\") == team_select)\n",").select(\n","    \"WEEK\",\n","    \"HOME_TEAM\",\n","    \"AWAY_TEAM\",\n","    \"HOME_TEAM_WIN_PERCENTAGE\",\n","    \"AWAY_TEAM_WIN_PERCENTAGE\",\n","    \"HOME_LAGGED_WIN_PERCENTAGE\",\n","    \"AWAY_LAGGED_WIN_PERCENTAGE\",\n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#Lets do a quick plot, how does the spread_line which is a forecast of the winners predict the home differential?\n","#Clearly in games where the home team is favored they are likely to have a higher point spread.\n","#Linear regression is not an appropriate modeling technique for this data, used here for illustration of trend.\n","\n","import seaborn as sns\n","pddf=schedule_updated.select(\"home_differential\",\"HOME_LAGGED_WIN_PERCENTAGE\")\\\n","                     .select(\"home_differential\", F.col(\"HOME_LAGGED_WIN_PERCENTAGE\").alias(\"HOME_LAGGED_WIN_PERCENTAGE\")).to_pandas()\n","\n","sns.set(style=\"darkgrid\", context=\"talk\", rc={'figure.figsize':(12.7,9.27)})\n","\n","# Create the plot using seaborn's regplot function\n","ax = sns.regplot(data=pddf, x=\"HOME_LAGGED_WIN_PERCENTAGE\", y=\"HOME_DIFFERENTIAL\", color='#00b3b3')\n","\n","# Add gridlines and remove the top and right spines\n","ax.grid(visible=True, alpha=0.5)\n","ax.spines['top'].set_visible(False)\n","ax.spines['right'].set_visible(False)\n","\n","# Set the x and y-axis labels\n","ax.set_xlabel(\"HOME LAGGED WIN PERCENTAGE\", fontsize=12, fontweight='bold')\n","ax.set_ylabel(\"HOME DIFFERENTIAL\", fontsize=12, fontweight='bold')\n","\n","# Set the x and y-axis tick marks\n","ax.tick_params(axis='both', which='major', labelsize=12)\n","\n","# Add a title to the chart\n","plt.title(\"Home Differential vs. Home Lagged Win Percentage\", fontsize=16, fontweight='bold')\n","plt.style.use(\"dark_background\")\n","\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#Which Teams Perform better at home? Lets look at point differential by team.\n","\n","pddf=schedule_updated.select(F.col(\"home_differential\"),\"HOME_TEAM\").to_pandas()\n","\n","sns.set(style=\"ticks\", context=\"talk\", rc={'figure.figsize':(12.7,12.7)})\n","\n","# Use the \"dark_background\" style for the plot\n","plt.style.use(\"dark_background\")\n","\n","# Group the data by the HOME_TEAM column and calculate the median home differential for each team\n","median_values = pddf.groupby(by=['HOME_TEAM'])['HOME_DIFFERENTIAL'].median()\n","\n","# Sort the median values in descending order and get the index (team names)\n","median_order = median_values.sort_values(ascending=False).iloc[::-1].index\n","\n","\n","ax = sns.boxplot(data=pddf, x=\"HOME_DIFFERENTIAL\", y=\"HOME_TEAM\", orient='h', order=median_order)\n","\n","# Change the x-axis label to \"Home Differential\"\n","ax.set_xlabel(\"Home Differential\")\n","\n","# Change the y-axis label to \"Home Team\"\n","ax.set_ylabel(\"Home Team\")\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Engineering\n","\n","Compute the offensive and defensive statistics for each game from our play by play table.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#Lets Compute some passing statistics to better understand each teams performance\n","#Join our Play By Play table to our passing table.\n","full_stats = (\n","    pbp.join(pbp_pass, pbp_pass.pbp_id == pbp.pbp_id, rsuffix=\"_throwaway\")\n","    .drop(\"PBP_ID_THROWAWAY\")\n","    .select(\n","        \"POSTEAM\",\n","        \"DEFTEAM\",\n","        \"GAME_ID\",\n","        \"PLAY_ID\",\n","        \"GAME_DATE\",\n","        \"WEEK\",\n","        \"YARDS_GAINED\",\n","        \"TOUCHDOWN\",\n","        \"INCOMPLETE_PASS\",\n","        \"FUMBLE_LOST\",\n","        \"INTERCEPTION\",\n","        \"SACK\",\n","        \"HOME_TEAM\",\n","        \"AWAY_TEAM\",\n","    )\n",")\n","\n","#Lets create some offensive and defensive stats\n","\n","# OFFENSIVE\n","yards_gained = F.sum(F.col(\"YARDS_GAINED\")).alias(\"YARDS_GAINED\")\n","touchdown = F.sum(F.col(\"TOUCHDOWN\")).alias(\"TOUCHDOWNS\")\n","incomplete_pass = F.sum(F.col(\"INCOMPLETE_PASS\")).alias(\"INCOMPLETE_PASS\")\n","fumble_lost = F.sum(F.col(\"FUMBLE_LOST\")).alias(\"FUMBLES_LOST\")\n","interception = F.sum(F.col(\"INTERCEPTION\")).alias(\"INTERCEPTIONS\")\n","sack = F.sum(F.col(\"SACK\")).alias(\"SACKS\")\n","\n","# DEFENSIVE\n","yards_allowed = F.sum(F.col(\"YARDS_GAINED\")).alias(\"YARDS_ALLOWED\")\n","touchdowns_allowed = F.sum(F.col(\"TOUCHDOWN\")).alias(\"TOUCHDOWNS_ALLOWED\")\n","incomplete_pass_forced = F.sum(F.col(\"INCOMPLETE_PASS\")).alias(\"INCOMPLETE_PASS_FORCED\")\n","fumbles_recovered = F.sum(F.col(\"FUMBLE_LOST\")).alias(\"FUMBLES_RECOVERED\")\n","interceptions_forced = F.sum(F.col(\"INTERCEPTION\")).alias(\"INTERCEPTIONS_FORCED\")\n","sacks_forced = F.sum(F.col(\"SACK\")).alias(\"SACKS_FORCED\")\n","\n","\n","sum_offense_stats = [\n","    yards_gained,\n","    touchdown,\n","    incomplete_pass,\n","    fumble_lost,\n","    interception,\n","    sack,\n","]\n","sum_defence_stats = [\n","    yards_allowed,\n","    touchdowns_allowed,\n","    incomplete_pass_forced,\n","    fumbles_recovered,\n","    interceptions_forced,\n","    sacks_forced,\n","]\n","\n","offensive_stats = full_stats.group_by(\"GAME_ID\", \"GAME_DATE\", \"POSTEAM\", \"WEEK\").agg(\n","    sum_offense_stats\n",")\n","\n","defensive_stats = full_stats.group_by(\"GAME_ID\", \"GAME_DATE\", \"DEFTEAM\", \"WEEK\").agg(\n","    sum_defence_stats\n",")\n","\n","join_condition1 = offensive_stats.POSTEAM == defensive_stats.DEFTEAM\n","join_condition2 = offensive_stats.GAME_ID == defensive_stats.GAME_ID\n","\n","all_stats=offensive_stats.join(defensive_stats, join_condition1 & join_condition2, rsuffix=\"_THROWAWAY\").drop(\"GAME_ID_THROWAWAY\",\"GAME_DATE_THROWAWAY\",\"WEEK_THROWAWAY\",\"DEFTEAM\")\n","\n","all_stats.filter(\n","    (F.col(\"POSTEAM\") == team_select)).order_by(\"WEEK\").show(5)"]},{"cell_type":"markdown","metadata":{},"source":["# Rolling Statistics\n","\n","Now Lets Create a rolling sum of our statistics across every game played. We will normalize our data on a per game played basis. As the season wears on we should see that our statics vary less, and are more reflective of a teams true potential. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["window_stats = (\n","    Window.partitionBy(\"POSTEAM\")\n","    .orderBy(\"GAME_DATE\")\n","    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",")\n","games_played = F.count(F.col(\"GAME_DATE\")).over(window_stats)\n","\n","stats_features = [\n","    \"YARDS_GAINED\",\n","    \"TOUCHDOWNS\",\n","    \"INCOMPLETE_PASS\",\n","    \"FUMBLES_LOST\",\n","    \"INTERCEPTIONS\",\n","    \"SACKS\",\n","    \"YARDS_ALLOWED\",\n","    \"TOUCHDOWNS_ALLOWED\",\n","    \"INCOMPLETE_PASS_FORCED\",\n","    \"FUMBLES_RECOVERED\",\n","    \"INTERCEPTIONS_FORCED\",\n","    \"SACKS_FORCED\",\n","]\n","\n","\n","# Calculate average values of statistics for team's offense and defense\n","\n","all_features = [\n","    (F.sum(F.col(x)).over(window_stats) / F.col(\"games_played\")).alias(x)\n","    for x in stats_features\n","]\n","\n","cumulative_statistics_columns = [\n","    F.col(\"POSTEAM\"),\n","    F.col(\"GAME_DATE\"),\n","    F.col(\"GAME_ID\"),\n","    F.col(\"WEEK\"),\n","] + all_features\n","\n","cumulative_features = all_stats.with_column(\"GAMES_PLAYED\", games_played).select(\n","    cumulative_statistics_columns\n",")\n","\n","cumulative_features.filter(F.col(\"POSTEAM\")==team_select).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["## Compute Lagged Statistics\n","\n","\n","\n","Because we are trying to predict the final score of a game before a game happens we must compute features for each game as they exist going into each game. We want to take our features table and add a new column called \"NEXT_GAME_ID\" which will let us join our schedule table to our Features table to compute what each teams statistics were before any given game began.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#We want to find the next game ID, we use a window function by team ordered by week to get this value\n","lag_window = Window.partition_by(\"POSTEAM\").order_by(F.col(\"WEEK\").asc())\n","\n","#Apply our window function and add a new column for the new game id\n","lead_features = cumulative_features.with_column(\n","    \"NEXT_GAME_ID\", F.lead(F.col(\"GAME_ID\")).over(lag_window)\n",")\n","\n","#Show our results\n","lead_features.filter(F.col(\"POSTEAM\")==team_select).order_by(\n","    F.col(\"GAME_ID\").desc()\n",").select(\"POSTEAM\",\"GAME_DATE\",\"WEEK\",\"GAME_ID\",\"NEXT_GAME_ID\").order_by(F.col(\"WEEK\")).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["### Join Features and Schedule Table\n","\n","Now we want to join our features and our schedule table together. This will give us a dataframe that shows a teams statistics at the start of any game. This will allow us to train our machine learning model to predict the spread of a game given the home team and away teams statistics at the time of game start.\n","\n","\n","\n","We must do this once for the home team, and once for the away team. We also need to make sure that \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Ensure that our final point spread is positive when the home team wins, and negative when the home team loses\n","condition_home = F.when(\n","    F.col(\"POSTEAM\") == F.col(\"HOME_TEAM\"), F.col(\"HOME_DIFFERENTIAL\")\n",").otherwise(-1 * F.col(\"HOME_DIFFERENTIAL\"))\n","\n","condition_away = F.when(\n","    F.col(\"POSTEAM\") == F.col(\"HOME_TEAM\"), -1 * F.col(\"HOME_DIFFERENTIAL\")\n",").otherwise(F.col(\"HOME_DIFFERENTIAL\"))\n","\n","home = (\n","    schedule_updated.join(\n","        lead_features,\n","        (schedule_updated.GAME_ID == lead_features.NEXT_GAME_ID)\n","        & (schedule_updated.HOME_TEAM == lead_features.POSTEAM),\n","        rsuffix=\"_THROWAWAY\",\n","    )\n","    .drop(\"GAME_ID_THROWAWAY\")\n","    .with_column(\"target\", condition_home)\n",")\n","\n","away = (\n","    schedule_updated.join(\n","        lead_features,\n","        (schedule_updated.GAME_ID == lead_features.NEXT_GAME_ID)\n","        & (schedule_updated.AWAY_TEAM == lead_features.POSTEAM),\n","        rsuffix=\"_THROWAWAY\",\n","    )\n","    .drop(\"GAME_ID_THROWAWAY\")\n","    .with_column(\"target\", condition_away)\n",")\n","\n","# We can immediately drop some features we no longer need\n","cols_to_drop = [\n","    \"NEXT_GAME_ID\",\n","    \"WEEK_THROWAWAY\",\n","]\n","\n","#We also have some features that apply to the game, but not each individual team. We do not need to duplicate these features\n","#So we can drop our duplicates\n","duplicate_feature_drop = [\n","    \"SPREAD_ODDS\",\n","    \"MONEYLINE\",\n","    \"POSTEAM\",\n","    \"GAME_DATE\",\n","    \"TARGET\",\n","    \"HOME_DIFFERENTIAL\",\n","    \"SCORE_DIFFERENTIAL\",\n","    \"WEEK\",\n","    \"GAMEDAY\",\n","    \"AWAY_TEAM\",\n","    \"HOME_TEAM\",\n","    \"AWAY_MONEYLINE\",\n","    \"HOME_MONEYLINE\",\n","    \"SPREAD_LINE\",\n","    \"AWAY_SPREAD_ODDS\",\n","    \"HOME_SPREAD_ODDS\",\n","    \"DIV_GAME\",\n","    \"ROOF\",\n","    \"SURFACE\",\n","    \"AWAY_COACH\",\n","    \"HOME_COACH\",\n","    \"WINNER\",\n","    \"AWAY_LAGGED_WIN_PERCENTAGE\",\n","    \"HOME_LAGGED_WIN_PERCENTAGE\",\n","    \"AWAY_TEAM_WIN_PERCENTAGE\",\n","    \"WINNER\",\n","    \"LOSER\",\n","    \"POSTEAM\",\n","    \"HOME_TEAM_WIN_PERCENTAGE\",\n","    \"AWAY_TEAM_WIN_PERCENTAGE\"\n","]\n","home = home.drop(cols_to_drop)\n","away = away.drop(cols_to_drop).drop(duplicate_feature_drop)\n","\n","# Rename our AWAY columns so they can be distinguished\n","new_names = [F.col(x).alias(f\"{x}_away\") for x in away.columns]\n","\n","away = away.select(new_names)\n","\n","df = home.join(away, home.GAME_ID == away.GAME_ID_AWAY, rsuffix=\"_THROWAWAY\")\n","df = df.drop(\"GAME_ID_AWAY\",\"SCORE_DIFFERENTIAL\",\"POSTEAM\")"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["df.filter((F.col(\"HOME_TEAM\")==team_select) | (F.col(\"AWAY_TEAM\")==team_select)).order_by(\"WEEK\").show(18)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["split_week = _hex_json.loads(\"15\")"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["test = df.filter(F.col(\"WEEK\") >= split_week)\n","train = df.filter(F.col(\"WEEK\") < split_week)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["train.order_by(\"GAME_ID\").show(3)\n","test.order_by(\"GAME_ID\").show(3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plot_options = [\"YARDS_GAINED\",\"TOUCHDOWNS\",\"INCOMPLETE_PASS\",\"FUMBLES_LOST\",\"INTERCEPTIONS\",\"SACKS\",\"YARDS_ALLOWED\",\"TOUCHDOWNS_ALLOWED\",\"INCOMPLETE_PASS_FORCED\",\"FUMBLES_RECOVERED\",\"INTERCEPTIONS_FORCED\",\"SACKS_FORCED\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import json as _hex_json\n","plot_variable = _hex_pks.kernel_execution.input_cell.run_dropdown_dynamic(args=_hex_types.DropdownDynamicArgs.from_dict({**_hex_json.loads(\"{\\\"dataframe_column\\\":null,\\\"ui_selected_value\\\":\\\"TOUCHDOWNS_ALLOWED\\\"}\"), **{_hex_json.loads(\"\\\"options_variable\\\"\"):_hex_kernel.variable_or_none(\"plot_options\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()))\n","\n","import json as _hex_json\n","_hex_pks.kernel_execution.input_cell.filled_dynamic_value(args=_hex_types.FilledDynamicValueArgs.from_dict({**_hex_json.loads(\"{\\\"variable_name\\\":\\\"plot_options\\\",\\\"dataframe_column\\\":null,\\\"max_size\\\":10000,\\\"max_size_in_bytes\\\":5242880}\"), **{_hex_json.loads(\"\\\"variable\\\"\"):_hex_kernel.variable_or_none(\"plot_options\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()))"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["pddf=lead_features.select(F.col(plot_variable),\"POSTEAM\").to_pandas()\n","pddf[plot_variable] = pddf[plot_variable].astype(float)\n","\n","sns.set(style=\"ticks\", context=\"talk\", rc={'figure.figsize':(12.7,12.7)})\n","\n","# Use the \"dark_background\" style for the plot\n","plt.style.use(\"dark_background\")\n","\n","# Group the data by the HOME_TEAM column and calculate the median home differential for each team\n","median_values = pddf.groupby(by=['POSTEAM'])[plot_variable].median()\n","\n","# Sort the median values in descending order and get the index (team names)\n","median_order = median_values.sort_values(ascending=False).iloc[::-1].index\n","\n","\n","ax = sns.boxplot(data=pddf, x=plot_variable, y=\"POSTEAM\", orient='h', order=median_order)\n","\n","# Change the x-axis label to \"Home Differential\"\n","ax.set_xlabel(plot_variable)\n","\n","# Change the y-axis label to \"Home Team\"\n","ax.set_ylabel(\"Team\")\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#Can plot either normalized version or un normalized version. \n","#Either way distributions should look similar.\n","metric = plot_variable\n","\n","sns.set(style=\"ticks\", context=\"talk\")\n","plt.style.use(\"dark_background\")\n","\n","train_pd = train.select(metric).to_pandas()\n","test_pd = test.select(metric).to_pandas()\n","\n","train_pd[metric]=train_pd[metric].astype(float)\n","test_pd[metric]=test_pd[metric].astype(float)\n","\n","\n","train_pd[metric].plot.kde()\n","test_pd[metric].plot.kde()\n","\n","\n","# Add a legend\n","plt.legend([\"Train\", \"Test\"])\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["train.create_or_replace_view(\"PC_HEX_DB.PUBLIC.NFL_DATA_TRAIN\")\n","nfl_data = hex_snowpark_session.table('PC_HEX_DB.PUBLIC.NFL_DATA_TRAIN')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["nfl_data.order_by(F.col(\"HOME_LAGGED_WIN_PERCENTAGE\").asc()).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#Now Leats Create our Model features_list\n","\n","features_to_train = [\n","    \"WEEK\",\n","    \"YARDS_GAINED\",\n","    \"YARDS_ALLOWED\",\n","    \"YARDS_GAINED_AWAY\",\n","    \"YARDS_ALLOWED_AWAY\",\n","    \"AWAY_MONEYLINE\",\n","    \"HOME_MONEYLINE\",\n","    #\"SPREAD_LINE\",\n","    F.div0(F.col(\"TOUCHDOWNS\"),F.col(\"TOUCHDOWNS_AWAY\")).alias(\"touchdown_ratio\"),\n","    F.div0(F.col(\"YARDS_GAINED\"),F.col(\"YARDS_ALLOWED\")).alias(\"yards_ratio\"),\n","    F.div0(F.col(\"YARDS_GAINED_AWAY\"),F.col(\"YARDS_ALLOWED_AWAY\")).alias(\"yards_away_ratio\"),\n","    \"SACKS\",\n","    \"FUMBLES_LOST\",\n","    \"FUMBLES_RECOVERED\",\n","    \"FUMBLES_LOST_AWAY\",\n","    \"FUMBLES_RECOVERED_AWAY\",\n","    \"SACKS_FORCED\",\n","    \"SACKS_AWAY\",\n","    \"SACKS_FORCED_AWAY\",\n","    \"TOUCHDOWNS\",\n","    \"TOUCHDOWNS_AWAY\",\n","    \"TOUCHDOWNS_ALLOWED\",\n","    \"TOUCHDOWNS_ALLOWED_AWAY\",\n","    \"INTERCEPTIONS\",\n","    \"INTERCEPTIONS_FORCED\",\n","    \"HOME_TEAM_WIN_PERCENTAGE\",\n","    \"AWAY_TEAM_WIN_PERCENTAGE\",\n","    \"TARGET\",\n","]\n","nfl_data = nfl_data.select(features_to_train)\n","print(nfl_data.columns)\n","\n","feature_drop_down = nfl_data.columns\n","\n","num_features=len(nfl_data.columns)-1"]},{"cell_type":"markdown","metadata":{},"source":["### Train Our Model\n","\n","\n","\n","Use a stored procedure to train our xgboost model. This will leverage a single node to train our model. For training multiple models in parallel, construct a UDTF. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from snowflake.snowpark.types import Variant\n","from snowflake.snowpark.functions import sproc\n","hex_snowpark_session.clear_imports()\n","\n","\n","hex_snowpark_session.sql\n","\n","hex_snowpark_session.sql(\"USE SCHEMA PC_HEX_DB.PUBLIC\").show()\n","hex_snowpark_session.sql(\"CREATE OR REPLACE STAGE  model_nfl_stage\").show()\n","\n","hex_snowpark_session.add_packages(\n","    *[\"snowflake-snowpark-python\", \"scikit-learn\", \"joblib\", \"xgboost\", \"numpy\"]\n",")\n","\n","\n","@sproc(\n","    session=hex_snowpark_session,\n","    name=\"train_xgb_nfl\",\n","    is_permanent=True,\n","    stage_location=\"@model_nfl_stage\",\n","    replace=True,\n",")\n","def train_xgb_nfl(session: Session, features_table: str) -> Variant:\n","    import xgboost as xgb\n","    from sklearn.model_selection import train_test_split\n","    import os\n","    from joblib import dump\n","    import numpy as np\n","    from sklearn.metrics import mean_squared_error as MSE\n","    from sklearn.metrics import r2_score as r2_score\n","\n","    df_in = session.table(features_table)\n","\n","    features_to_train = [\n","        \"YARDS_GAINED\",\n","        \"YARDS_ALLOWED\",\n","        \"YARDS_GAINED_AWAY\",\n","        \"YARDS_ALLOWED_AWAY\",\n","        \"AWAY_MONEYLINE\",\n","        \"HOME_MONEYLINE\",\n","        # \"SPREAD_LINE\",\n","        F.div0(F.col(\"TOUCHDOWNS\"), F.col(\"TOUCHDOWNS_AWAY\")).alias(\"touchdown_ratio\"),\n","        F.div0(F.col(\"YARDS_GAINED\"), F.col(\"YARDS_ALLOWED\")).alias(\"yards_ratio\"),\n","        F.div0(F.col(\"YARDS_GAINED_AWAY\"), F.col(\"YARDS_ALLOWED_AWAY\")).alias(\n","            \"yards_away_ratio\"\n","        ),\n","        \"SACKS\",\n","        \"FUMBLES_LOST\",\n","        \"FUMBLES_RECOVERED\",\n","        \"FUMBLES_LOST_AWAY\",\n","        \"FUMBLES_RECOVERED_AWAY\",\n","        \"SACKS_FORCED\",\n","        \"SACKS_AWAY\",\n","        \"SACKS_FORCED_AWAY\",\n","        \"TOUCHDOWNS\",\n","        \"TOUCHDOWNS_AWAY\",\n","        \"TOUCHDOWNS_ALLOWED\",\n","        \"TOUCHDOWNS_ALLOWED_AWAY\",\n","        \"INTERCEPTIONS\",\n","        \"INTERCEPTIONS_FORCED\",\n","        \"HOME_TEAM_WIN_PERCENTAGE\",\n","        \"AWAY_TEAM_WIN_PERCENTAGE\",\n","        \"TARGET\",\n","    ]\n","\n","    df_in = df_in.select(features_to_train)\n","\n","    df_in = df_in.to_pandas()\n","    num_features = len(df_in.columns) - 1\n","\n","    X = df_in.iloc[:, :num_features].to_numpy()\n","    Y = df_in.iloc[:, num_features:].to_numpy()\n","    Y = np.ravel(Y)\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, Y, random_state=42, test_size=0.1\n","    )\n","\n","    D_train = xgb.DMatrix(X_train, label=y_train)\n","    D_test = xgb.DMatrix(X_test, label=y_test)\n","\n","    model = xgb.XGBRegressor(\n","        n_estimators=2000,\n","        max_depth=10,\n","        eta=0.001,\n","        subsample=0.5,\n","        colsample_bytree=0.8,\n","        reg_lambda=0.5,\n","        reg_alpha=0.5,\n","        gamma=100,\n","    )\n","\n","    model.fit(X_train, y_train)\n","\n","    # Predict the model\n","    pred = model.predict(X_test)\n","\n","    # RMSE Computation\n","    rmse = np.sqrt(MSE(y_test, pred))\n","    r2 = r2_score(y_test, pred)\n","\n","    model_file = os.path.join('/tmp', 'model_nfl.joblib')\n","    dump(model, model_file)\n","    session.file.put(model_file, \"@model_nfl_stage\",overwrite=True)\n","\n","    return {\"R2_Test\": str(r2), \"RMSE_Test\": str(rmse)}"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["hex_snowpark_session.call('train_xgb_nfl',\"PC_HEX_DB.PUBLIC.NFL_DATA_TRAIN\")"]},{"cell_type":"markdown","metadata":{},"source":["### Inference Model\n","\n","\n","\n","Now that we have a trained model we will build a user defined function that will run inference on our model across multiple nodes. This will enable us to compute large batch inference jobs very quickly and scales well with increasing cluster size. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Now. Lets define a model udf\n","from snowflake.snowpark.functions import udf\n","hex_snowpark_session.sql(\"USE SCHEMA PC_HEX_DB.PUBLIC\").show()\n","\n","hex_snowpark_session.clear_imports()\n","\n","# Add our model to import into our user defined function\n","hex_snowpark_session.add_import('@model_nfl_stage/model_nfl.joblib.gz')\n","\n","hex_snowpark_session.add_packages(\n","    *[\"snowflake-snowpark-python\", \"scikit-learn\", \"joblib\", \"xgboost\", \"numpy\",\"pandas\"]\n",")\n","#Create a user defined function, using a decorator or a udf function. Define where this will be stored.\n","@udf(\n","    name=\"predict_xgb_nfl\",\n","    session=hex_snowpark_session,\n","    replace=True,\n","    is_permanent=True,\n","    stage_location=\"@model_nfl_stage\"\n",")\n","def predict_xgb_nfl(args: list) -> float:\n","    import xgboost\n","    import pandas as pd\n","    from joblib import load\n","    import sys\n","\n","    features = [\n","        \"YARDS_GAINED\",\n","        \"YARDS_ALLOWED\",\n","        \"YARDS_GAINED_AWAY\",\n","        \"YARDS_ALLOWED_AWAY\",\n","        \"AWAY_MONEYLINE\",\n","        \"HOME_MONEYLINE\",\n","        \"TOUCHDOWN_RATIO\",\n","        \"YARDS_RATIO\",\n","        \"YARDS_AWAY_RATIO\",\n","        \"SACKS\",\n","        \"FUMBLES_LOST\",\n","        \"FUMBLES_RECOVERED\",\n","        \"FUMBLES_LOST_AWAY\",\n","        \"FUMBLES_RECOVERED_AWAY\",\n","        \"SACKS_FORCED\",\n","        \"SACKS_AWAY\",\n","        \"SACKS_FORCED_AWAY\",\n","        \"TOUCHDOWNS\",\n","        \"TOUCHDOWNS_AWAY\",\n","        \"TOUCHDOWNS_ALLOWED\",\n","        \"TOUCHDOWNS_ALLOWED_AWAY\",\n","        \"INTERCEPTIONS\",\n","        \"INTERCEPTIONS_FORCED\",\n","        \"HOME_TEAM_WIN_PERCENTAGE\",\n","        \"AWAY_TEAM_WIN_PERCENTAGE\"\n","    ]\n","\n","    # Compute Feature Dataframe\n","    args = pd.DataFrame([args], columns=features)\n","\n","    # Reference our MOdel\n","    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n","    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n","    model_file = import_dir + \"model_nfl.joblib.gz\"\n","    model = load(model_file)\n","\n","    prediction = model.predict(args)[0]\n","\n","    return prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["    #Now we can run inference on our model and predict games \n","    # This is distributed Inference and each node will process a fraction of our data in parallel. \n","    features_lst = [\n","        \"YARDS_GAINED\",\n","        \"YARDS_ALLOWED\",\n","        \"YARDS_GAINED_AWAY\",\n","        \"YARDS_ALLOWED_AWAY\",\n","        \"AWAY_MONEYLINE\",\n","        \"HOME_MONEYLINE\",\n","        \"TOUCHDOWN_RATIO\",\n","        \"YARDS_RATIO\",\n","        \"YARDS_AWAY_RATIO\",\n","        \"SACKS\",\n","        \"FUMBLES_LOST\",\n","        \"FUMBLES_RECOVERED\",\n","        \"FUMBLES_LOST_AWAY\",\n","        \"FUMBLES_RECOVERED_AWAY\",\n","        \"SACKS_FORCED\",\n","        \"SACKS_AWAY\",\n","        \"SACKS_FORCED_AWAY\",\n","        \"TOUCHDOWNS\",\n","        \"TOUCHDOWNS_AWAY\",\n","        \"TOUCHDOWNS_ALLOWED\",\n","        \"TOUCHDOWNS_ALLOWED_AWAY\",\n","        \"INTERCEPTIONS\",\n","        \"INTERCEPTIONS_FORCED\",\n","        \"HOME_TEAM_WIN_PERCENTAGE\",\n","        \"AWAY_TEAM_WIN_PERCENTAGE\"\n","    ]\n","features = list(map(F.col,features_lst))\n","features=F.array_construct(*features)\n","\n","\n","#use the Call udf function to define a function object that can be applied to columns.\n","nfl_data.select(features_to_train).with_column('predictions', F.call_udf('predict_xgb_nfl', features)).select(\"TARGET\",\"PREDICTIONS\").show()\n","\n","#features_to_train"]},{"cell_type":"markdown","metadata":{},"source":["Import our Model To our Local App for Plotting\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from joblib import load\n","\n","#Download our model to our local environment\n","hex_snowpark_session.sql(\"USE SCHEMA PC_HEX_DB.PUBLIC\").show()\n","\n","hex_snowpark_session.file.get('@model_nfl_stage/model_nfl.joblib.gz','.')\n","\n","xgb_model=load('model_nfl.joblib.gz')"]},{"cell_type":"markdown","metadata":{},"source":["Plot Feature Importance\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["\n","from xgboost import plot_importance\n","\n","\n","sns.set(style=\"ticks\", context=\"talk\")\n","plot_importance(xgb_model).set_yticklabels(features_lst)\n","\n","plt.show()\n","\n"]}],"metadata":{"hex_info":{"author":"Dominick Rocco","exported_date":"Wed May 10 2023 02:41:19 GMT+0000 (Coordinated Universal Time)","project_id":"aa5041b0-f9b6-4d0e-96d6-4f22abb91763","version":"import"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":4}
